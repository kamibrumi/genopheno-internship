{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Source Tutorial: https://docs.dgl.ai/guide/training-node.html\n",
    "\n",
    "### Data Set: https://docs.dgl.ai/api/python/dgl.data.html#dgl.data.QM9EdgeDataset\n",
    "# Loading QM9EdgeDataset\n",
    "from dgl.data import SSTDataset\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "\n",
    "dataset = SSTDataset() # Can set multiple label keys per graph\n",
    "for tree in dataset:\n",
    "    input_ids = tree.ndata['x']\n",
    "    labels = tree.ndata['y']\n",
    "    mask = tree.ndata['mask']\n",
    "\n",
    "    # graph, label = data[0]\n",
    "#     fig, ax = plt.subplots()\n",
    "#     nx.draw(tree.to_networkx(), ax=ax)\n",
    "#     print(\"example graph:\")\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "###############################################################################\n",
    "# Form a graph mini-batch\n",
    "# -----------------------\n",
    "# To train neural networks efficiently, a common practice is to batch\n",
    "# multiple samples together to form a mini-batch. Batching fixed-shaped tensor\n",
    "# inputs is common. For example, batching two images of size 28 x 28\n",
    "# gives a tensor of shape 2 x 28 x 28. By contrast, batching graph inputs\n",
    "# has two challenges:\n",
    "#\n",
    "# * Graphs are sparse.\n",
    "# * Graphs can have various length. For example, number of nodes and edges.\n",
    "#\n",
    "# To address this, DGL provides a :func:`dgl.batch` API. It leverages the idea that\n",
    "# a batch of graphs can be viewed as a large graph that has many disjointed \n",
    "# connected components. Below is a visualization that gives the general idea.\n",
    "#\n",
    "# .. image:: https://data.dgl.ai/tutorial/batch/batch.png\n",
    "#     :width: 400pt\n",
    "#     :align: center\n",
    "#\n",
    "# Define the following ``collate`` function to form a mini-batch from a given\n",
    "# list of graph and label pairs.\n",
    "\n",
    "import dgl\n",
    "import torch\n",
    "\n",
    "def collate(samples):\n",
    "    # The input `samples` is a list of pairs\n",
    "    #  (graph, label).\n",
    "    new_samples = []\n",
    "    for graph, _ in samples:\n",
    "        new_samples.append((graph, _))\n",
    "    graphs, labels = map(list, zip(*new_samples))\n",
    "     \n",
    "    batched_graph = dgl.batch(graphs)\n",
    "    return batched_graph, torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# Readout and classification\n",
    "# --------------------------\n",
    "# For this demonstration, consider initial node features to be their degrees.\n",
    "# After two rounds of graph convolution, perform a graph readout by averaging\n",
    "# over all node features for each graph in the batch.\n",
    "#\n",
    "# .. math::\n",
    "#\n",
    "#    h_g=\\frac{1}{|\\mathcal{V}|}\\sum_{v\\in\\mathcal{V}}h_{v}\n",
    "#\n",
    "# In DGL, :func:`dgl.mean_nodes` handles this task for a batch of\n",
    "# graphs with variable size. You then feed the graph representations into a\n",
    "# classifier with one linear layer to obtain pre-softmax logits.\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, n_classes):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.conv1 = GraphConv(in_dim, hidden_dim)\n",
    "        self.conv2 = GraphConv(hidden_dim, hidden_dim)\n",
    "        self.classify = nn.Linear(hidden_dim, n_classes)\n",
    "\n",
    "    def forward(self, g):\n",
    "        # Use node degree as the initial node feature. For undirected graphs, the in-degree\n",
    "        # is the same as the out_degree.\n",
    "        h = g.in_degrees().view(-1, 1).float()\n",
    "        # Perform graph convolution and activation function.\n",
    "        h = F.relu(self.conv1(g, h))\n",
    "        h = F.relu(self.conv2(g, h))\n",
    "        g.ndata['h'] = h\n",
    "        # Calculate graph representation by averaging all the node representations.\n",
    "        hg = dgl.mean_nodes(g, 'h')\n",
    "        return self.classify(hg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide train and test set\n",
    "\n",
    "train_data = SSTDataset()\n",
    "dev_data = SSTDataset(mode='dev')\n",
    "test_data = SSTDataset(mode='test')\n",
    "tiny_data = SSTDataset(mode='tiny')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "###############################################################################\n",
    "# Setup and training\n",
    "# ------------------\n",
    "###############################################################################\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from dgl.nn.pytorch import GraphConv\n",
    "import numpy as np\n",
    "\n",
    "# Create training and test sets.\n",
    "# Use PyTorch's DataLoader and the collate function\n",
    "# defined before.\n",
    "\n",
    "# Tunes specific parameters (currently only learning rate and hidden dimensions)\n",
    "# Outputs the MSE and Epoch losses array\n",
    "def tuning_round(curr_lr, curr_epoch_range, curr_batch_size):\n",
    "    # 4/15: Made batch_size to 1\n",
    "    data_loader = DataLoader(train_data, batch_size=curr_batch_size, shuffle=True,\n",
    "                             collate_fn=collate)\n",
    "\n",
    "    # Create model\n",
    "    #modified to contain a single output\n",
    "    #model = Classifier(1, 256, trainset.num_classes)\n",
    "    #TODO: need to train hyperparameters hidden layers\n",
    "    # hidden_dim = 256\n",
    "    model = Classifier(1, 256, 1)\n",
    "    loss_func = nn.MSELoss()\n",
    "\n",
    "    #modifications to migrate to regression\n",
    "    #loss_func = nn.CrossEntropyLoss()\n",
    "    #optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=curr_lr)\n",
    "    model.train()\n",
    "\n",
    "    epoch_losses = [] # MSE average list\n",
    "    epoch_r_losses = [] # RMSE average list\n",
    "    for epoch in range(curr_epoch_range):\n",
    "        epoch_loss = 0\n",
    "        epoch_r_loss = 0\n",
    "        for iter, (bg, label) in enumerate(data_loader):\n",
    "            prediction = model(bg)\n",
    "            print(prediction)\n",
    "            loss = loss_func(prediction, label)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # Mean Relative Error\n",
    "            MRE_loss = np.average(np.abs(np.array(prediction.detach().numpy()) - np.array(label.detach().numpy()))/np.abs(np.array(label.detach().numpy())))\n",
    "            epoch_loss += loss.detach().item()\n",
    "            if np.isinf(MRE_loss):\n",
    "                print(bg)\n",
    "                print(label)\n",
    "            print(MRE_loss)\n",
    "            epoch_r_loss += MRE_loss\n",
    "        epoch_loss /= (iter + 1)\n",
    "        epoch_r_loss /= (iter + 1)\n",
    "\n",
    "        epoch_losses.append(epoch_loss)\n",
    "        epoch_r_losses.append(epoch_r_loss)\n",
    "    return epoch_losses, epoch_r_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "DGLError",
     "evalue": "Invalid key \"0\". Must be one of the edge types.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mDGLError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-5128e8decfa1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mlr_tuning\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0.02\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlr_tuning\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mepoch_losses_arr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_r_losses_arr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtuning_round\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcurr_epoch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcurr_batch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m     \u001b[0mmse_lr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch_losses_arr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch_losses_arr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mrmse_lr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch_r_losses_arr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch_r_losses_arr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-13-51ed58c8e36e>\u001b[0m in \u001b[0;36mtuning_round\u001b[1;34m(curr_lr, curr_epoch_range, curr_batch_size)\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[0mepoch_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[0mepoch_r_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0miter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mbg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m             \u001b[0mprediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    433\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    434\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 435\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    436\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    437\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    473\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    474\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 475\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    476\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    477\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-8-2c4d87db41ce>\u001b[0m in \u001b[0;36mcollate\u001b[1;34m(samples)\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[1;31m#  (graph, label).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[0mnew_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mgraph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msamples\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m         \u001b[0mnew_samples\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[0mgraphs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mnew_samples\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\dgl\\heterograph.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2150\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2151\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0metypes\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2152\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mDGLError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Invalid key \"{}\". Must be one of the edge types.'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morig_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2154\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0metypes\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mDGLError\u001b[0m: Invalid key \"0\". Must be one of the edge types."
     ]
    }
   ],
   "source": [
    "# Tuning Learning Rate\n",
    "# TODO: grid searching for all the hyperparameters\n",
    "#       Currently not exhaustively searching through all the hyperparameters\n",
    "mse_lr_arr = []\n",
    "rmse_lr_arr = []\n",
    "curr_epoch_size = 30\n",
    "curr_batch_size = 32\n",
    "lr_tuning = [0.02]\n",
    "for i in lr_tuning:\n",
    "    epoch_losses_arr, epoch_r_losses_arr = tuning_round(i, curr_epoch_size, curr_batch_size)\n",
    "    mse_lr = sum(epoch_losses_arr)/len(epoch_losses_arr)\n",
    "    rmse_lr = sum(epoch_r_losses_arr)/len(epoch_r_losses_arr)\n",
    "    mse_lr_arr.append(mse_lr)\n",
    "    rmse_lr_arr.append(rmse_lr)\n",
    "\n",
    "best_lr = lr_tuning[mse_lr_arr.index(min(mse_lr_arr))]\n",
    "\n",
    "print(\"Optimized Learning Rate: \", best_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'mse_lr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-374c1b2bbf51>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch_losses_arr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Average MSE: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmse_lr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'MRE averaged over minibatches (n = 32)'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'mse_lr' is not defined"
     ]
    }
   ],
   "source": [
    "###############################################################################\n",
    "# The learning curve of a run with the best learning rate is presented below.\n",
    "###############################################################################\n",
    "import matplotlib.pyplot as plt\n",
    "# epoch_losses_arr, epoch_r_losses_arr = tuning_round(best_lr, 10, 32)\n",
    "# mse_lr = sum(epoch_losses_arr)/len(epoch_losses_arr)\n",
    "# rmse_lr = sum(epoch_r_losses_arr)/len(epoch_r_losses_arr)\n",
    "\n",
    "plt.title('MSE averaged over minibatches (n = 32)')\n",
    "plt.plot(epoch_losses_arr)\n",
    "plt.show()\n",
    "print(\"Average MSE: \", mse_lr)\n",
    "\n",
    "plt.title('MRE averaged over minibatches (n = 32)')\n",
    "plt.plot(epoch_r_losses_arr)\n",
    "plt.show()\n",
    "print(epoch_r_losses_arr)\n",
    "# print(\"Average MRE: \", rmse_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 45.],\n",
      "        [ 30.],\n",
      "        [ 37.],\n",
      "        [ 37.],\n",
      "        [ 81.],\n",
      "        [ 91.],\n",
      "        [109.],\n",
      "        [106.],\n",
      "        [ 80.],\n",
      "        [ 80.],\n",
      "        [ 68.],\n",
      "        [ 28.],\n",
      "        [289.],\n",
      "        [289.],\n",
      "        [ 72.],\n",
      "        [ 40.],\n",
      "        [ 56.],\n",
      "        [ 72.],\n",
      "        [ 72.],\n",
      "        [ 72.]])\n",
      "tensor([[74.3363],\n",
      "        [74.3363],\n",
      "        [61.2663],\n",
      "        [61.2663],\n",
      "        [64.4156],\n",
      "        [63.8844],\n",
      "        [69.0286],\n",
      "        [68.5445],\n",
      "        [68.4744],\n",
      "        [68.4744],\n",
      "        [71.9507],\n",
      "        [72.6212],\n",
      "        [83.5326],\n",
      "        [83.5326],\n",
      "        [71.4144],\n",
      "        [71.4144],\n",
      "        [71.4144],\n",
      "        [71.4144],\n",
      "        [71.4144],\n",
      "        [71.4144]], grad_fn=<AddmmBackward>)\n",
      "MSE:  0.4559594\n"
     ]
    }
   ],
   "source": [
    "# The trained model is evaluated on the test set created. \n",
    "model.eval()\n",
    "\n",
    "# Convert a list of tuples to two lists\n",
    "new_samples = []\n",
    "for graph, _ in testset:\n",
    "    new_samples.append((graph, _))\n",
    "test_X, test_Y = map(list, zip(*new_samples))\n",
    "\n",
    "test_bg = dgl.batch(test_X)\n",
    "test_Y = torch.tensor(test_Y).float().view(-1, 1)\n",
    "print(\"Ground truth:\", test_Y)\n",
    "\n",
    "# probs_Y = torch.softmax(model(test_bg), 1)\n",
    "probs_Y = model(test_bg)\n",
    "print(\"Predictions: \", probs_Y)\n",
    "total_sum = 0\n",
    "for i, r in enumerate(test_X):\n",
    "    total_sum += probs_Y[i][0].item() - r.number_of_edges()\n",
    "# test_X, test_Y = map(list, zip(*testset))\n",
    "# # print(model)\n",
    "# exit()\n",
    "# sampled_Y = torch.multinomial(probs_Y, 1)\n",
    "# argmax_Y = torch.max(probs_Y, 1)[1].view(-1, 1)\n",
    "# print('Accuracy of sampled predictions on the test set: {:.4f}%'.format(\n",
    "#     (test_Y == probs_Y.float()).sum().item() / len(test_Y) * 100))\n",
    "# print('Accuracy of argmax predictions on the test set: {:4f}%'.format(\n",
    "#     (test_Y == argmax_Y.float()).sum().item() / len(test_Y) * 100))\n",
    "\n",
    "print(\"MRE: \", np.average(np.abs(np.array(probs_Y.detach().numpy()) - np.array(test_Y.detach().numpy()))/abs(np.array(test_Y.detach().numpy()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
