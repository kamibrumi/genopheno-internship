{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n.. currentmodule:: dgl\n\nGraph Classification Tutorial\n=============================\n\n**Author**: `Mufei Li <https://github.com/mufeili>`_,\n`Minjie Wang <https://jermainewang.github.io/>`_,\n`Zheng Zhang <https://shanghai.nyu.edu/academics/faculty/directory/zheng-zhang>`_.\n\nIn this tutorial, you learn how to use DGL to batch multiple graphs of variable size and shape. The \ntutorial also demonstrates training a graph neural network for a simple graph classification task.\n\nGraph classification is an important problem\nwith applications across many fields, such as bioinformatics, chemoinformatics, social\nnetwork analysis, urban computing, and cybersecurity. Applying graph neural\nnetworks to this problem has been a popular approach recently. This can be seen in the following reserach references: \n`Ying et al., 2018 <https://arxiv.org/abs/1806.08804>`_,\n`Cangea et al., 2018 <https://arxiv.org/abs/1811.01287>`_,\n`Knyazev et al., 2018 <https://arxiv.org/abs/1811.09595>`_,\n`Bianchi et al., 2019 <https://arxiv.org/abs/1901.01343>`_,\n`Liao et al., 2019 <https://arxiv.org/abs/1901.01484>`_,\n`Gao et al., 2019 <https://openreview.net/forum?id=HJePRoAct7>`_).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Simple graph classification task\n--------------------------------\nIn this tutorial, you learn how to perform batched graph classification\nwith DGL. The example task objective is to classify eight types of topologies shown here.\n\n![](https://data.dgl.ai/tutorial/batch/dataset_overview.png)\n\n    :align: center\n\nImplement a synthetic dataset :class:`data.MiniGCDataset` in DGL. The dataset has eight \ndifferent types of graphs and each class has the same number of graph samples.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from dgl.data import MiniGCDataset\nimport matplotlib.pyplot as plt\nimport networkx as nx\n# A dataset with 80 samples, each graph is\n# of size [10, 20]\ndataset = MiniGCDataset(80, 10, 20)\ngraph, label = dataset[0]\nfig, ax = plt.subplots()\nnx.draw(graph.to_networkx(), ax=ax)\nax.set_title('Class: {:d}'.format(label))\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Form a graph mini-batch\n-----------------------\nTo train neural networks efficiently, a common practice is to batch\nmultiple samples together to form a mini-batch. Batching fixed-shaped tensor\ninputs is common. For example, batching two images of size 28 x 28\ngives a tensor of shape 2 x 28 x 28. By contrast, batching graph inputs\nhas two challenges:\n\n* Graphs are sparse.\n* Graphs can have various length. For example, number of nodes and edges.\n\nTo address this, DGL provides a :func:`dgl.batch` API. It leverages the idea that\na batch of graphs can be viewed as a large graph that has many disjointed \nconnected components. Below is a visualization that gives the general idea.\n\n![](https://data.dgl.ai/tutorial/batch/batch.png)\n\n    :width: 400pt\n    :align: center\n\nDefine the following ``collate`` function to form a mini-batch from a given\nlist of graph and label pairs.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import dgl\nimport torch\n\ndef collate(samples):\n    # The input `samples` is a list of pairs\n    #  (graph, label).\n    graphs, labels = map(list, zip(*samples))\n    batched_graph = dgl.batch(graphs)\n    return batched_graph, torch.tensor(labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The return type of :func:`dgl.batch` is still a graph. In the same way, \na batch of tensors is still a tensor. This means that any code that works\nfor one graph immediately works for a batch of graphs. More importantly,\nbecause DGL processes messages on all nodes and edges in parallel, this greatly\nimproves efficiency.\n\nGraph classifier\n----------------\nGraph classification proceeds as follows.\n\n![](https://data.dgl.ai/tutorial/batch/graph_classifier.png)\n\n\nFrom a batch of graphs, perform message passing and graph convolution\nfor nodes to communicate with others. After message passing, compute a\ntensor for graph representation from node (and edge) attributes. This step might \nbe called readout or aggregation. Finally, the graph \nrepresentations are fed into a classifier $g$ to predict the graph labels.\n\nGraph convolution layer can be found in the ``dgl.nn.<backend>`` submodule.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from dgl.nn.pytorch import GraphConv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Readout and classification\n--------------------------\nFor this demonstration, consider initial node features to be their degrees.\nAfter two rounds of graph convolution, perform a graph readout by averaging\nover all node features for each graph in the batch.\n\n\\begin{align}h_g=\\frac{1}{|\\mathcal{V}|}\\sum_{v\\in\\mathcal{V}}h_{v}\\end{align}\n\nIn DGL, :func:`dgl.mean_nodes` handles this task for a batch of\ngraphs with variable size. You then feed the graph representations into a\nclassifier with one linear layer to obtain pre-softmax logits.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\nimport torch.nn.functional as F\n\nclass Classifier(nn.Module):\n    def __init__(self, in_dim, hidden_dim, n_classes):\n        super(Classifier, self).__init__()\n        self.conv1 = GraphConv(in_dim, hidden_dim)\n        self.conv2 = GraphConv(hidden_dim, hidden_dim)\n        self.classify = nn.Linear(hidden_dim, n_classes)\n\n    def forward(self, g):\n        # Use node degree as the initial node feature. For undirected graphs, the in-degree\n        # is the same as the out_degree.\n        h = g.in_degrees().view(-1, 1).float()\n        # Perform graph convolution and activation function.\n        h = F.relu(self.conv1(g, h))\n        h = F.relu(self.conv2(g, h))\n        g.ndata['h'] = h\n        # Calculate graph representation by averaging all the node representations.\n        hg = dgl.mean_nodes(g, 'h')\n        return self.classify(hg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Setup and training\n------------------\nCreate a synthetic dataset of $400$ graphs with $10$ ~\n$20$ nodes. $320$ graphs constitute a training set and\n$80$ graphs constitute a test set.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\nfrom torch.utils.data import DataLoader\n\n# Create training and test sets.\ntrainset = MiniGCDataset(320, 10, 20)\ntestset = MiniGCDataset(80, 10, 20)\n# Use PyTorch's DataLoader and the collate function\n# defined before.\ndata_loader = DataLoader(trainset, batch_size=32, shuffle=True,\n                         collate_fn=collate)\n\n# Create model\nmodel = Classifier(1, 256, trainset.num_classes)\nloss_func = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\nmodel.train()\n\nepoch_losses = []\nfor epoch in range(80):\n    epoch_loss = 0\n    for iter, (bg, label) in enumerate(data_loader):\n        prediction = model(bg)\n        loss = loss_func(prediction, label)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        epoch_loss += loss.detach().item()\n    epoch_loss /= (iter + 1)\n    print('Epoch {}, loss {:.4f}'.format(epoch, epoch_loss))\n    epoch_losses.append(epoch_loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The learning curve of a run is presented below.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plt.title('cross entropy averaged over minibatches')\nplt.plot(epoch_losses)\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The trained model is evaluated on the test set created. To deploy\nthe tutorial, restrict the running time to get a higher\naccuracy ($80$ % ~ $90$ %) than the ones printed below.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model.eval()\n# Convert a list of tuples to two lists\ntest_X, test_Y = map(list, zip(*testset))\ntest_bg = dgl.batch(test_X)\ntest_Y = torch.tensor(test_Y).float().view(-1, 1)\nprobs_Y = torch.softmax(model(test_bg), 1)\nsampled_Y = torch.multinomial(probs_Y, 1)\nargmax_Y = torch.max(probs_Y, 1)[1].view(-1, 1)\nprint('Accuracy of sampled predictions on the test set: {:.4f}%'.format(\n    (test_Y == sampled_Y.float()).sum().item() / len(test_Y) * 100))\nprint('Accuracy of argmax predictions on the test set: {:4f}%'.format(\n    (test_Y == argmax_Y.float()).sum().item() / len(test_Y) * 100))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The animation here plots the probability that a trained model predicts the correct graph type.\n\n![](https://data.dgl.ai/tutorial/batch/test_eval4.gif)\n\n\nTo understand the node and graph representations that a trained model learned,\nwe use `t-SNE, <https://lvdmaaten.github.io/tsne/>`_ for dimensionality reduction\nand visualization.\n\n![](https://data.dgl.ai/tutorial/batch/tsne_node2.png)\n\n    :align: center\n\n![](https://data.dgl.ai/tutorial/batch/tsne_graph2.png)\n\n    :align: center\n\nThe two small figures on the top separately visualize node representations after one and two\nlayers of graph convolution. The figure on the bottom visualizes\nthe pre-softmax logits for graphs as graph representations.\n\nWhile the visualization does suggest some clustering effects of the node features,\nyou would not expect a perfect result. Node degrees are deterministic for\nthese node features. The graph features are improved when separated.\n\nWhat's next?\n------------\nGraph classification with graph neural networks is still a new field.\nIt's waiting for people to bring more exciting discoveries. The work requires \nmapping different graphs to different embeddings, while preserving\ntheir structural similarity in the embedding space. To learn more about it, see \n`How Powerful Are Graph Neural Networks? <https://arxiv.org/abs/1810.00826>`_ a research paper  \npublished for the International Conference on Learning Representations 2019.\n\nFor more examples about batched graph processing, see the following:\n\n* Tutorials for `Tree LSTM <https://docs.dgl.ai/tutorials/models/2_small_graph/3_tree-lstm.html>`_ and `Deep Generative Models of Graphs <https://docs.dgl.ai/tutorials/models/3_generative_model/5_dgmg.html>`_\n* An example implementation of `Junction Tree VAE <https://github.com/dmlc/dgl/tree/master/examples/pytorch/jtnn>`_\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}